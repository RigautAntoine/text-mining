{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, nltk, itertools, sklearn\n",
    "import numpy as np\n",
    "from nltk.corpus import treebank\n",
    "from collections import Counter\n",
    "from nltk.classify import MaxentClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = list(treebank.tagged_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [(w, t) for w, t in corpus if t not in ['-NONE-', '-RRB-', '-LRB-']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = [t for w, t in corpus]\n",
    "words = [w for w, t in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent vs rare words in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2475 frequent words and 9489 rare words in training.\n"
     ]
    }
   ],
   "source": [
    "wordfreq = nltk.FreqDist(words).most_common()\n",
    "frequent = [w for w, c in wordfreq if c >= 5]\n",
    "rare = [w for w, c in wordfreq if c < 5]\n",
    "\n",
    "print('We have %d frequent words and %d rare words in training.' % (len(frequent), len(rare)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rareset = [(w, t) for w, t in zip(words, tags) if w in rare]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the rare corpus is 14769\n"
     ]
    }
   ],
   "source": [
    "print('The size of the rare corpus is %d' % len(rareset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " ('join', 'VB'),\n",
       " ('Vinken', 'NNP'),\n",
       " ('Elsevier', 'NNP')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rareset[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefixes and suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prefixes = Counter([w[0:i].lower() for w, t in rareset for i in range(1, 5) if w.isalpha() and len(w) > 4]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1524 candidate prefixes\n"
     ]
    }
   ],
   "source": [
    "candidates = [pr for pr, c in prefixes if c >= 5]\n",
    "\n",
    "print('%d candidate prefixes' % len(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ig(feature_set, ws, labels):\n",
    "    # Calculate the entropy of a Counter object, a dictionary of (class, count) pairs,\n",
    "    # with s equal to the total count. \n",
    "    \n",
    "    N = float(len(labels))\n",
    "\n",
    "    def entropy(count_dict, s):\n",
    "        ts = [count_dict[label]/s * np.log2(count_dict[label]/s) for label in count_dict.keys()]\n",
    "        return sum(ts)\n",
    "        \n",
    "    # Calculate the gain from a given word\n",
    "    def gain(prefix):\n",
    "        # Calculate the entropy of the set of reviews when w is in the review\n",
    "        # This entropy is multiplied by the probability of w\n",
    "        prefix_in_set = Counter([label for label, w in zip(labels, ws) if prefix in [w[0:i].lower() for i in range(1, 5)]])\n",
    "        s_in_set = sum(prefix_in_set.values())\n",
    "        entropy_in_set = entropy(prefix_in_set, s_in_set) * s_in_set / N\n",
    "        # Calculate the entropy of the set of reviews when w is not in the review        \n",
    "        prefix_not_in_set = Counter([label for label, w in zip(labels, ws) if prefix not in [w[0:i].lower() for i in range(1, 5)]])\n",
    "        s_not_in_set = sum(prefix_not_in_set.values())\n",
    "        entropy_not_in_set = entropy(prefix_not_in_set, s_not_in_set) * s_not_in_set / N\n",
    "        # The sum of class entropy and the two entropies makes the gain\n",
    "        return class_entropy + entropy_in_set + entropy_not_in_set\n",
    "    \n",
    "    # Calculate the entropy of the corpus\n",
    "    class_entropy = -entropy(Counter(labels), N)\n",
    "    ig_scores = []\n",
    "    for feature in feature_set:\n",
    "        ig_scores.append((feature, gain(feature)))\n",
    "    return ig_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ig_scores = ig(candidates, [w for w, t in rareset], [t for w, t in rareset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ig_scores = sorted(ig_scores, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('re', 0.018572926248108246),\n",
       " ('c', 0.01581403394539338),\n",
       " ('s', 0.015478164455462462),\n",
       " ('p', 0.014231028979970528),\n",
       " ('r', 0.014045551638506737),\n",
       " ('u', 0.012183629968901322),\n",
       " ('b', 0.011791700659400384),\n",
       " ('a', 0.011212705927205135),\n",
       " ('un', 0.010751169259090254),\n",
       " ('m', 0.010535547978256066)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ig_scores[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132 candidate suffixes.\n"
     ]
    }
   ],
   "source": [
    "suffixes = Counter([w[-i:].lower() for w, t in rareset for i in range(1, 5) if w.isalpha() and len(w) > 4]).most_common()\n",
    "candidates = [p for p, c in suffixes if c >= 5]\n",
    "\n",
    "print('%d candidate suffixes.' % len(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ig(feature_set, ws, labels):\n",
    "    # Calculate the entropy of a Counter object, a dictionary of (class, count) pairs,\n",
    "    # with s equal to the total count. \n",
    "    \n",
    "    N = float(len(labels))\n",
    "\n",
    "    def entropy(count_dict, s):\n",
    "        ts = [count_dict[label]/s * np.log2(count_dict[label]/s) for label in count_dict.keys()]\n",
    "        return sum(ts)\n",
    "        \n",
    "    # Calculate the gain from a given word\n",
    "    def gain(suffix):\n",
    "        # Calculate the entropy of the set of reviews when w is in the review\n",
    "        # This entropy is multiplied by the probability of w\n",
    "        suffix_in_set = Counter([label for label, w in zip(labels, ws) if suffix in [w[-i:].lower() for i in range(1, 5)]])\n",
    "        s_in_set = sum(suffix_in_set.values())\n",
    "        entropy_in_set = entropy(suffix_in_set, s_in_set) * s_in_set / N\n",
    "        # Calculate the entropy of the set of reviews when w is not in the review        \n",
    "        suffix_not_in_set = Counter([label for label, w in zip(labels, ws) if suffix not in [w[-i:].lower() for i in range(1, 5)]])\n",
    "        s_not_in_set = sum(suffix_not_in_set.values())\n",
    "        entropy_not_in_set = entropy(suffix_not_in_set, s_not_in_set) * s_not_in_set / N\n",
    "        # The sum of class entropy and the two entropies makes the gain\n",
    "        return class_entropy + entropy_in_set + entropy_not_in_set\n",
    "    \n",
    "    # Calculate the entropy of the corpus\n",
    "    class_entropy = -entropy(Counter(labels), N)\n",
    "    ig_scores = []\n",
    "    for feature in feature_set:\n",
    "        ig_scores.append((feature, gain(feature)))\n",
    "    return ig_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ig_scores2 = ig(candidates, [w for w, t in rareset], [t for w, t in rareset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ig_scores2 = sorted(ig_scores2, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 0.48460878400949658),\n",
       " ('ed', 0.30264209203916659),\n",
       " ('d', 0.2645901646878821),\n",
       " ('ing', 0.22843062383353363),\n",
       " ('ng', 0.22376797866087195),\n",
       " ('g', 0.22012741361743782),\n",
       " ('ly', 0.11826558116849384),\n",
       " ('es', 0.11013502876243297),\n",
       " ('y', 0.10328141511898803),\n",
       " ('e', 0.091312708165125933),\n",
       " ('rs', 0.069583697078176687),\n",
       " ('ted', 0.065480553533864949),\n",
       " ('ts', 0.064016197702356337),\n",
       " ('r', 0.052517821627971362),\n",
       " ('n', 0.051689131243768838),\n",
       " ('ers', 0.05141280360775502),\n",
       " ('t', 0.048043972535427137),\n",
       " ('ting', 0.047435192132976756),\n",
       " ('on', 0.044013522820365303),\n",
       " ('ion', 0.043646994609473744),\n",
       " ('er', 0.042113134829287802),\n",
       " ('ns', 0.041119722345153598),\n",
       " ('tion', 0.037397858544500284),\n",
       " ('l', 0.03448849853192959),\n",
       " ('lly', 0.03396912339646585),\n",
       " ('ally', 0.03165722717036612),\n",
       " ('ons', 0.029700602830903033),\n",
       " ('ions', 0.028275033009527828),\n",
       " ('al', 0.027520059472585956),\n",
       " ('a', 0.024044403755754562),\n",
       " ('red', 0.02393994040807268),\n",
       " ('ies', 0.023512349723991299),\n",
       " ('ated', 0.022961700943423313),\n",
       " ('ely', 0.021409604152892392),\n",
       " ('ds', 0.020570121596136026),\n",
       " ('st', 0.019574381064787261),\n",
       " ('ls', 0.019370132514394012),\n",
       " ('ned', 0.019313037733724947),\n",
       " ('sed', 0.01931027579409772),\n",
       " ('nts', 0.019209498613189435),\n",
       " ('led', 0.018954209509321363),\n",
       " ('te', 0.018395813422179774),\n",
       " ('nt', 0.018235800927429491),\n",
       " ('le', 0.017165102189048209),\n",
       " ('est', 0.016658711085974165),\n",
       " ('ble', 0.016068979035211317),\n",
       " ('ded', 0.015734879023574955),\n",
       " ('ling', 0.015222860798594251),\n",
       " ('i', 0.014470039001694524),\n",
       " ('ent', 0.014063081097746011),\n",
       " ('o', 0.014061198552244303),\n",
       " ('ring', 0.013899991589815475),\n",
       " ('ic', 0.013842681087620878),\n",
       " ('ty', 0.013816640294945781),\n",
       " ('ve', 0.013803447661679957),\n",
       " ('an', 0.013683153645290158),\n",
       " ('c', 0.013648570588326692),\n",
       " ('ding', 0.013418299876998496),\n",
       " ('able', 0.013047506774390705),\n",
       " ('ents', 0.012852117289616327),\n",
       " ('h', 0.012594383273687004),\n",
       " ('tly', 0.012525876112108758),\n",
       " ('ning', 0.012212619806649005),\n",
       " ('ive', 0.011818445297469005),\n",
       " ('ors', 0.011755801127052212),\n",
       " ('gs', 0.011678343487724341),\n",
       " ('ks', 0.011391492403424852),\n",
       " ('ate', 0.011248880814495266),\n",
       " ('k', 0.011146585600772418),\n",
       " ('ry', 0.011006282975175807),\n",
       " ('ered', 0.010939420277330303),\n",
       " ('ters', 0.010647758440374577),\n",
       " ('ngs', 0.010446371159973822),\n",
       " ('tes', 0.010301290736578483),\n",
       " ('sts', 0.010288988234651075),\n",
       " ('ment', 0.010200483226812551),\n",
       " ('nds', 0.010186640347372045),\n",
       " ('ity', 0.010152800338044887),\n",
       " ('nd', 0.010080128895178397),\n",
       " ('re', 0.0099716586908287219),\n",
       " ('m', 0.0099617307711916681),\n",
       " ('ings', 0.0097238498078286995),\n",
       " ('ce', 0.0096957242724480253),\n",
       " ('or', 0.0096557125269423061),\n",
       " ('tely', 0.0096319404499229933),\n",
       " ('ces', 0.0095993254369690639),\n",
       " ('ving', 0.009338003228548164),\n",
       " ('ked', 0.0091436689062840415),\n",
       " ('les', 0.0089135100292945246),\n",
       " ('als', 0.0089091385286677038),\n",
       " ('ties', 0.0088930809606804218),\n",
       " ('ses', 0.0088836052230307772),\n",
       " ('cted', 0.0088291249227081536),\n",
       " ('ge', 0.0086442571619937958),\n",
       " ('hed', 0.0085933996906195631),\n",
       " ('se', 0.0085275939137092749),\n",
       " ('tors', 0.0084283572219177039),\n",
       " ('ous', 0.0084072194163291591),\n",
       " ('nted', 0.0083083014719118609),\n",
       " ('ied', 0.0081980451560714762)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ig_scores2[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "legal_prefixes = [pr for pr, score in ig_scores if score > 0.01]\n",
    "legal_suffixes = [pr for pr, score in ig_scores2 if score > 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 79)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(legal_prefixes), len(legal_suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upper = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "lower = 'abcdefghijklmnopqrstuvwxyz'\n",
    "mapping = dict((l, 'X') for l in upper)\n",
    "mapping.update(dict((l, 'x') for l in lower))\n",
    "mapping.update(dict((d, 'd') for d in '0123456789'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordshape(w, mapping = mapping):\n",
    "    def mapper(letter):\n",
    "        if letter in mapping:\n",
    "            return mapping[letter]\n",
    "        else:\n",
    "            return letter\n",
    "    return ''.join(mapper(l) for l in w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def short_wordshape(w, mapping=mapping):\n",
    "    long = wordshape(w, mapping)\n",
    "    prev = long[0]\n",
    "    short = prev\n",
    "    for l in long[1:]:\n",
    "        if l != prev:\n",
    "            prev = l\n",
    "            short += prev\n",
    "    return short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ig(feature_set, ws, labels):\n",
    "    # Calculate the entropy of a Counter object, a dictionary of (class, count) pairs,\n",
    "    # with s equal to the total count. \n",
    "    \n",
    "    N = float(len(labels))\n",
    "\n",
    "    def entropy(count_dict, s):\n",
    "        ts = [count_dict[label]/s * np.log2(count_dict[label]/s) for label in count_dict.keys()]\n",
    "        return sum(ts)\n",
    "        \n",
    "    # Calculate the gain from a given word\n",
    "    def gain(shape):\n",
    "        # Calculate the entropy of the set of reviews when w is in the review\n",
    "        # This entropy is multiplied by the probability of w\n",
    "        shape_in_set = Counter([label for label, w in zip(labels, ws) if shape == wordshape(w)])\n",
    "        s_in_set = sum(shape_in_set.values())\n",
    "        entropy_in_set = entropy(shape_in_set, s_in_set) * s_in_set / N\n",
    "        # Calculate the entropy of the set of reviews when w is not in the review        \n",
    "        shape_not_in_set = Counter([label for label, w in zip(labels, ws) if shape != wordshape(w)])\n",
    "        s_not_in_set = sum(shape_not_in_set.values())\n",
    "        entropy_not_in_set = entropy(shape_not_in_set, s_not_in_set) * s_not_in_set / N\n",
    "        # The sum of class entropy and the two entropies makes the gain\n",
    "        return class_entropy + entropy_in_set + entropy_not_in_set\n",
    "    \n",
    "    # Calculate the entropy of the corpus\n",
    "    class_entropy = -entropy(Counter(labels), N)\n",
    "    ig_scores = []\n",
    "    for feature in feature_set:\n",
    "        ig_scores.append((feature, gain(feature)))\n",
    "    return ig_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shapes = Counter([wordshape(w) for w, t in rareset]).most_common()\n",
    "candidates = [shape for shape, count in shapes if count >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ig_score_shape = ig(candidates, [w for w, t in rareset], [t for w, t in rareset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ig_score_shape = sorted(ig_score_shape, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Xxxxxx', 0.065511138546845871),\n",
       " ('xxxxxxx', 0.062358958396059094),\n",
       " ('Xxxxx', 0.060549769672417764),\n",
       " ('Xxxxxxx', 0.059726230421480953),\n",
       " ('xxxxxx', 0.059357639303121434),\n",
       " ('xxxxxxxx', 0.056474484098516076),\n",
       " ('xxxxxxxxx', 0.050422977086370668),\n",
       " ('d.dd', 0.046854614223687641),\n",
       " ('xxxxx', 0.044827886833670672),\n",
       " ('ddd', 0.041882102348762551)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ig_score_shape[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ig(feature_set, ws, labels):\n",
    "    # Calculate the entropy of a Counter object, a dictionary of (class, count) pairs,\n",
    "    # with s equal to the total count. \n",
    "    \n",
    "    N = float(len(labels))\n",
    "\n",
    "    def entropy(count_dict, s):\n",
    "        ts = [count_dict[label]/s * np.log2(count_dict[label]/s) for label in count_dict.keys()]\n",
    "        return sum(ts)\n",
    "        \n",
    "    # Calculate the gain from a given word\n",
    "    def gain(shape):\n",
    "        # Calculate the entropy of the set of reviews when w is in the review\n",
    "        # This entropy is multiplied by the probability of w\n",
    "        shape_in_set = Counter([label for label, w in zip(labels, ws) if shape == short_wordshape(w)])\n",
    "        s_in_set = sum(shape_in_set.values())\n",
    "        entropy_in_set = entropy(shape_in_set, s_in_set) * s_in_set / N\n",
    "        # Calculate the entropy of the set of reviews when w is not in the review        \n",
    "        shape_not_in_set = Counter([label for label, w in zip(labels, ws) if shape != short_wordshape(w)])\n",
    "        s_not_in_set = sum(shape_not_in_set.values())\n",
    "        entropy_not_in_set = entropy(shape_not_in_set, s_not_in_set) * s_not_in_set / N\n",
    "        # The sum of class entropy and the two entropies makes the gain\n",
    "        return class_entropy + entropy_in_set + entropy_not_in_set\n",
    "    \n",
    "    # Calculate the entropy of the corpus\n",
    "    class_entropy = -entropy(Counter(labels), N)\n",
    "    ig_scores = []\n",
    "    for feature in feature_set:\n",
    "        ig_scores.append((feature, gain(feature)))\n",
    "    return ig_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shapes = Counter([short_wordshape(w) for w, t in rareset]).most_common()\n",
    "candidates = [shape for shape, count in shapes if count >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ig_score_shape2 = ig(candidates, [w for w, t in rareset], [t for w, t in rareset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ig_score_shape2 = sorted(ig_score_shape2, key=lambda x: -x[1] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('x', 0.59397088282276156),\n",
       " ('Xx', 0.44331098828203874),\n",
       " ('d.d', 0.17628408818709751),\n",
       " ('d', 0.08465020470904161),\n",
       " ('x-x', 0.075550385688024413),\n",
       " ('d,d', 0.03074741695201455),\n",
       " ('X', 0.016850989288419438),\n",
       " ('d-x', 0.0083882236320960146),\n",
       " ('Xx-x', 0.0066339574579599159),\n",
       " ('X.X.', 0.0060607364375004913)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ig_score_shape2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 7)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_shapes = [shape for shape, freq in ig_score_shape if freq > 0.01]\n",
    "legal_short_shapes = [shape for shape, freq in ig_score_shape2 if freq > 0.01]\n",
    "\n",
    "len(legal_shapes), len(legal_short_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def features(w):\n",
    "    feats = {}\n",
    "    if w in frequent:\n",
    "        feats['word'] = w\n",
    "    else:\n",
    "        for prefixe in [w[0:i].lower() for i in range(1, 5)]:\n",
    "            if prefixe in legal_prefixes:\n",
    "                feats['pref has ' + prefixe] = 1\n",
    "        for suffixe in [w[-i:].lower() for i in range(1, 5)]:\n",
    "            if suffixe in legal_suffixes:\n",
    "                feats['suff has ' + suffixe] = 1        \n",
    "        feats['shape'] = wordshape(w)\n",
    "        feats['short_shape'] = short_wordshape(w)\n",
    "        feats['has_upper'] = 1 if re.search('[A-Z]', w) else 0\n",
    "        feats['has_hyphen'] = 1 if re.search('[-]', w) else 0\n",
    "        feats['has_digit'] = 1 if re.search('[0-9]', w) else 0\n",
    "        feats['is_upper'] = 1 if w.isupper() else 0\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prev = 'START'\n",
    "featuresset = []\n",
    "for i in range(len(corpus)):\n",
    "    w = words[i]\n",
    "    y = tags[i]\n",
    "    feats = features(w)\n",
    "    feats['prevTAG'] = prev\n",
    "    featuresset.append((feats, y))\n",
    "    if y == '.':\n",
    "        prev = 'START'\n",
    "    else:\n",
    "        prev = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (20 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -3.63759        0.005\n",
      "             2          -1.46524        0.895\n",
      "             3          -0.92967        0.940\n",
      "             4          -0.68835        0.949\n",
      "             5          -0.54962        0.953\n",
      "             6          -0.46007        0.956\n",
      "             7          -0.39770        0.958\n",
      "             8          -0.35184        0.959\n",
      "             9          -0.31672        0.961\n",
      "            10          -0.28897        0.962\n",
      "            11          -0.26649        0.963\n",
      "            12          -0.24791        0.963\n",
      "            13          -0.23229        0.963\n",
      "            14          -0.21897        0.964\n",
      "            15          -0.20748        0.965\n",
      "            16          -0.19745        0.965\n",
      "            17          -0.18863        0.967\n",
      "            18          -0.18080        0.967\n",
      "            19          -0.17381        0.967\n",
      "         Final          -0.16752        0.967\n"
     ]
    }
   ],
   "source": [
    "model = MaxentClassifier.train(featuresset[0:10000], max_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  12.053 word=='whose' and label is 'WP$'\n",
      "  11.480 word=='Its' and label is 'PRP$'\n",
      "  11.248 word=='publishing' and label is 'VBG'\n",
      "  11.185 word=='when' and label is 'WRB'\n",
      "  11.065 word=='enough' and label is 'RB'\n",
      "  10.981 word=='where' and label is 'WRB'\n",
      "  10.686 word=='operate' and label is 'VBP'\n",
      "  10.672 word=='earlier' and label is 'JJR'\n",
      "  10.640 word=='More' and label is 'RBR'\n",
      "  10.524 word=='35' and label is 'CD'\n",
      "  10.524 word=='11' and label is 'CD'\n",
      "  10.439 word=='causing' and label is 'VBG'\n",
      "  10.379 word=='computer' and label is 'NN'\n",
      "  10.376 word=='what' and label is 'WP'\n",
      "  10.348 word=='who' and label is 'WP'\n",
      "  10.321 word=='preferred' and label is 'VBN'\n",
      "  10.265 word=='offering' and label is 'VBG'\n",
      "  10.195 word=='reporting' and label is 'VBG'\n",
      "  10.195 word=='stronger' and label is 'JJR'\n",
      "  10.102 word=='more' and label is 'RBR'\n",
      "  10.044 word=='least' and label is 'JJS'\n",
      "  10.031 word=='rather' and label is 'RB'\n",
      "  10.012 word=='likely' and label is 'JJ'\n",
      "   9.987 word=='why' and label is 'WRB'\n",
      "   9.985 word=='systems' and label is 'NNS'\n",
      "   9.975 word=='seeking' and label is 'VBG'\n",
      "   9.967 word=='greater' and label is 'JJR'\n",
      "   9.895 word=='lower' and label is 'JJR'\n",
      "   9.865 word=='using' and label is 'VBG'\n",
      "   9.848 word=='fewer' and label is 'JJR'\n",
      "   9.773 word=='there' and label is 'EX'\n",
      "   9.743 word=='most' and label is 'JJS'\n",
      "   9.734 word=='brokers' and label is 'NNS'\n",
      "   9.704 word=='higher' and label is 'JJR'\n",
      "   9.688 word=='raising' and label is 'VBG'\n",
      "   9.675 word=='When' and label is 'WRB'\n",
      "   9.650 word=='having' and label is 'VBG'\n",
      "   9.650 word=='involving' and label is 'VBG'\n",
      "   9.650 word=='sweeping' and label is 'VBG'\n",
      "   9.628 word=='`' and label is '``'\n",
      "   9.621 word=='composite' and label is 'JJ'\n",
      "   9.621 word=='pharmaceutical' and label is 'JJ'\n",
      "   9.571 word=='developing' and label is 'VBG'\n",
      "   9.503 word=='turn' and label is 'VB'\n",
      "   9.487 word=='less' and label is 'JJR'\n",
      "   9.481 word=='Dealers' and label is 'NNPS'\n",
      "   9.468 word=='better' and label is 'JJR'\n",
      "   9.377 word=='Indiana' and label is 'NNP'\n",
      "   9.370 word=='longer' and label is 'JJR'\n",
      "   9.370 word=='broader' and label is 'JJR'\n",
      "   9.350 word=='--' and label is ':'\n",
      "   9.348 word=='500' and label is 'CD'\n",
      "   9.326 word=='Oct.' and label is 'NNP'\n",
      "   9.306 word==':' and label is ':'\n",
      "   9.303 word=='Securities' and label is 'NNPS'\n",
      "   9.278 word=='came' and label is 'VBD'\n",
      "   9.231 word=='replaced' and label is 'VBN'\n",
      "   9.231 word=='maintained' and label is 'VBN'\n",
      "   9.227 word=='According' and label is 'VBG'\n",
      "   9.204 word=='There' and label is 'EX'\n",
      "   9.192 word=='remaining' and label is 'VBG'\n",
      "   9.192 word=='pending' and label is 'VBG'\n",
      "   9.192 word=='managing' and label is 'VBG'\n",
      "   9.169 word=='him' and label is 'PRP'\n",
      "   9.122 word=='themselves' and label is 'PRP'\n",
      "   9.103 word=='should' and label is 'MD'\n",
      "   9.075 word=='fixed' and label is 'VBN'\n",
      "   9.067 word=='showing' and label is 'VBG'\n",
      "   9.051 word=='Such' and label is 'JJ'\n",
      "   9.050 word==';' and label is ':'\n",
      "   9.048 word=='virtually' and label is 'RB'\n",
      "   9.045 word=='institutions' and label is 'NNS'\n",
      "   9.036 word=='holding' and label is 'VBG'\n",
      "   9.034 word=='filed' and label is 'VBN'\n",
      "   8.993 word=='...' and label is ':'\n",
      "   8.967 word=='Atlanta' and label is 'NNP'\n",
      "   8.955 word=='slowing' and label is 'VBG'\n",
      "   8.917 word=='once' and label is 'RB'\n",
      "   8.883 word=='Moreover' and label is 'JJR'\n",
      "   8.865 word=='launched' and label is 'VBN'\n",
      "   8.862 word=='relatively' and label is 'RB'\n",
      "   8.850 word=='Tuesday' and label is 'NNP'\n",
      "   8.816 word=='growing' and label is 'VBG'\n",
      "   8.814 word=='here' and label is 'RB'\n",
      "   8.801 word=='runs' and label is 'VBZ'\n",
      "   8.798 word=='which' and label is 'WDT'\n",
      "   8.794 word=='developed' and label is 'VBD'\n",
      "   8.793 word=='beginning' and label is 'VBG'\n",
      "   8.782 word=='six' and label is 'CD'\n",
      "   8.768 word==\"''\" and label is \"''\"\n",
      "   8.766 word=='publicly' and label is 'RB'\n",
      "   8.765 word=='related' and label is 'VBN'\n",
      "   8.732 word=='bonds' and label is 'NNS'\n",
      "   8.732 word=='recently' and label is 'RB'\n",
      "   8.731 word=='``' and label is '``'\n",
      "   8.728 word=='how' and label is 'WRB'\n",
      "   8.704 word=='nearly' and label is 'RB'\n",
      "   8.702 word=='fully' and label is 'RB'\n",
      "   8.684 word=='they' and label is 'PRP'\n",
      "   8.678 word=='12' and label is 'CD'\n"
     ]
    }
   ],
   "source": [
    "model.show_most_informative_features(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "sequences = []\n",
    "for i in range(len(corpus)):\n",
    "    if corpus[i][1] == '.':\n",
    "        sequences.append(corpus[start:i+1])\n",
    "        start = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = len(sequences)\n",
    "X = [[w for w, tag in sequence] for sequence in sequences]\n",
    "y = [[tag for w, tag in sequence] for sequence in sequences]\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MaxentClassifier.classify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(22, 1), (24, 1), (1457, 1), (26, 1), (1621, 1)]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._encoding.encode(, 'VB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = model.prob_classify(features(words[1118]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IN',\n",
       " 'NNPS',\n",
       " 'RB',\n",
       " 'NNS',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'VBP',\n",
       " 'VB',\n",
       " 'NNP',\n",
       " 'MD',\n",
       " ',',\n",
       " 'RBR',\n",
       " '.',\n",
       " '$',\n",
       " 'VBZ',\n",
       " 'JJR',\n",
       " 'VBN',\n",
       " '``',\n",
       " 'POS',\n",
       " 'JJS',\n",
       " 'WP',\n",
       " 'CD',\n",
       " 'WRB',\n",
       " 'RBS',\n",
       " 'WDT',\n",
       " 'WP$',\n",
       " 'JJ',\n",
       " 'PRP',\n",
       " ':',\n",
       " 'VBD',\n",
       " 'VBG',\n",
       " 'CC',\n",
       " 'PRP$',\n",
       " 'EX',\n",
       " 'PDT',\n",
       " 'TO',\n",
       " 'RP',\n",
       " \"''\"]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tagger(sequence, states, transition, emission):\n",
    "    T = len(sequence)\n",
    "    N = len(states)\n",
    "    treillis = np.zeros((N, T))\n",
    "    max_came_from = {}\n",
    "    \n",
    "    # Initialization\n",
    "    first_word = features(sequence[0])\n",
    "    for s in range(N):\n",
    "        treillis[s, 0] = transition((states[s], 'START')) * emission((sequence[0], states[s]))\n",
    "        max_came_from[s, 0] = 'START'\n",
    "    \n",
    "    # Recursion\n",
    "    for t in range(1, T):\n",
    "        for s in range(N):\n",
    "            em = emission((sequence[t], states[s]))\n",
    "            inputs = [transition((states[s], states[k])) * em * treillis[k, t-1] for k in range(N)]\n",
    "            treillis[s, t] = np.max(inputs)\n",
    "            max_came_from[s, t] = (np.argmax(inputs), t-1)\n",
    "    \n",
    "    # Termination\n",
    "    inputs = [treillis[k, -1] * transition(('.', states[k])) for k in range(N)]\n",
    "    max_came_from['END'] = (np.argmax(inputs), T-1)\n",
    "    \n",
    "    # Reconstruct the path\n",
    "    prev = max_came_from['END']\n",
    "    best_sequence = []\n",
    "    while prev != 'START':\n",
    "        best_sequence.append(states[prev[0]])\n",
    "        prev = max_came_from[prev]\n",
    "    return best_sequence[::-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
