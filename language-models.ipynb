{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, os, re, nltk, math\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_yelp_reviews(path_to_json):\n",
    "    raw_reviews = []; authors = []; dates = []\n",
    "    json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "    pattern = re.compile(\"\\xa0\")\n",
    "    for js in json_files:\n",
    "        with open(os.path.join(path_to_json, js)) as json_file:\n",
    "            json_text = json.load(json_file)\n",
    "            for review in json_text['Reviews']:\n",
    "                authors.append(review['Author'])\n",
    "                dates.append(review['Date'])\n",
    "                content = pattern.sub('', review['Content'])\n",
    "                raw_reviews.append(content)\n",
    "    return raw_reviews, authors, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, train_authors, train_dates = load_yelp_reviews('../data/yelp/train')\n",
    "test, test_authors, test_dates = load_yelp_reviews('../data/yelp/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer\n",
    "stemmer = EnglishStemmer()\n",
    "tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents = [sent for text in train for sent in nltk.sent_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321626"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents = [split for sent in sents for split in re.split('[.!?]', sent) if split != '' and len(split) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378535"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = [' '.join(['<S>', sent, '<E>']) for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation = ['.', ',', '!', '?', '(', ')', ').', ';', ':', '\"', \"'\", \"''\", '\"\"', '``', '-', '/', '[', ']', '..', '...', '=', '*', '+']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = [word for sent in sents2 for word in tokenizer.tokenize(sent) if word not in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens2 = []\n",
    "for token in tokens:\n",
    "    if re.match('[0-9]+[\\.:,/]?[0-9]*[a-zA-Z]*', token) is None:\n",
    "        tokens2.append(token)\n",
    "    else:\n",
    "        tokens2.append('NUM')\n",
    "        if re.findall('[a-zA-Z]+', token):\n",
    "            tokens2.append(re.findall('[a-zA-Z]+', token)[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens2 = [stemmer.stem(token) if token != 'NUM' else token for token in tokens2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_counts = Counter(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hapaxes = [key for key in unigram_counts.keys() if unigram_counts[key] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hapax = fdist.hapaxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.562396006655574"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hapax) / fdist.B()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hapaxes represent 56% of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004119189804360562"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdist.hapaxes()) / len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But less 0.4% of the tokens. Most of them are typos and badly spelled and uncertain vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-fc4269e433b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokens3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'UNK'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhapax\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-145-fc4269e433b1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokens3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'UNK'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhapax\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokens3 = ['UNK' if token in hapax else token for token in tokens2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to build a unigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unigrams(tokens):\n",
    "    m = len(tokens)\n",
    "    return Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seen_unigrams = Counter(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = len(seen_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41469"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary size of 20,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bigram_counts(tokens):\n",
    "    return Counter([bigram for bigram in zip(tokens[0:-1], tokens[1:])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seen_bigrams = bigram_counts(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('<e>', '<s>'), 378534),\n",
       " (('<s>', 'i'), 49698),\n",
       " (('<s>', 'the'), 48795),\n",
       " (('it', 'was'), 18720),\n",
       " (('of', 'the'), 17669),\n",
       " (('<s>', 'we'), 17110),\n",
       " (('and', 'the'), 16442),\n",
       " (('this', 'place'), 12600),\n",
       " (('<s>', 'it'), 12170),\n",
       " (('in', 'the'), 11746)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen_bigrams.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the linearly interpolated models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bigram_MLE(tokens):\n",
    "    bigram_counts = get_bigram_counts(tokens)\n",
    "    unigram_counts = Counter(tokens)\n",
    "    mle = bigram_counts\n",
    "    for key in bigram_counts.keys():\n",
    "        mle[key] = bigram_counts[key] / unigram_counts[key[0]]\n",
    "    return mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigrams_MLE = get_bigram_MLE(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interpolated_bigrams = {}\n",
    "lmbda = 0.9\n",
    "for key in seen_bigrams.keys():\n",
    "    interpolated_bigrams[key] = (lmbda * bigrams_mle[key]) + ((1.0 - lmbda) * seen_unigram[key[1]] / V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discounted_bigrams = {}\n",
    "delta = 0.1\n",
    "for key in seen_bigrams.keys():\n",
    "    discounted_bigrams[key] = max(seen_bigrams[key] - delta, 0) / seen_unigrams[key[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best word after a given word according to a bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def best_next_word(word, model, nb_results=10):\n",
    "    results = []\n",
    "    for key in model.keys():\n",
    "        if key[0] == word:\n",
    "            results.append((key, model[key]))\n",
    "    results.sort(reverse=True, key = lambda x: x[1])\n",
    "    return results[0:nb_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('good', 'but'), 0.08257672058868064),\n",
       " (('good', 'and'), 0.06017736763477807),\n",
       " (('good', 'the'), 0.056356020591485645),\n",
       " (('good', 'i'), 0.04966389219957439),\n",
       " (('good', 'as'), 0.03289047298054568),\n",
       " (('good', 'food'), 0.026929445029091562),\n",
       " (('good', 'it'), 0.018845074461102677),\n",
       " (('good', 'thing'), 0.01663643418720738),\n",
       " (('good', 'for'), 0.01654124570483455),\n",
       " (('good', 'too'), 0.015325271265937305)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_next_word('good', interpolated_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('good', 'but'), 0.09066200671846901),\n",
       " (('good', 'and'), 0.06317736079535816),\n",
       " (('good', 'the'), 0.05659461843846494),\n",
       " (('good', 'i'), 0.05231922907264769),\n",
       " (('good', 'as'), 0.03606596315021547),\n",
       " (('good', 'food'), 0.029347494146788368),\n",
       " (('good', 'it'), 0.01896440568694649),\n",
       " (('good', 'thing'), 0.018319704115910555),\n",
       " (('good', 'for'), 0.017098164297105627),\n",
       " (('good', 'too'), 0.016792779342404397)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_next_word('good', discounted_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sentence(bigrams):\n",
    "    \n",
    "    # Sample the start of the sentence by sampling from the bigrams starting with <s>\n",
    "    previous = '<s>'\n",
    "    sentence = [previous]\n",
    "    \n",
    "    biwords = list(bigrams.keys())\n",
    "    biprobs = list(bigrams.values())\n",
    "    l = len(biwords)\n",
    "    end_not_seen = True\n",
    "    \n",
    "    while end_not_seen:\n",
    "        p = np.random.uniform(0.0, 1.0, 1)[0]\n",
    "        j = 0\n",
    "        i = 0\n",
    "        while j < p and i < l:\n",
    "            if biwords[i][0] == previous:\n",
    "                j += biprobs[i]\n",
    "            i += 1\n",
    "        previous = biwords[max(i - 1, 0)][1]\n",
    "        sentence.append(previous)\n",
    "        prob = prob * biprobs[i - 1]\n",
    "        if previous == '<e>':\n",
    "            end_not_seen = False\n",
    "    \n",
    "    print(' '.join(sentence), \"Likelihood = \" + str(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "while communal dine with i be share to the dessert sampler was serv cake Likelihood = 4.71697655412332e-35\n",
      "the owner and it i guess we wander michigan ave it was bad cold Likelihood = 6.55065235388781e-28\n",
      "with squid dish was a bit of drink and walk in the did you Likelihood = 5.786848646047065e-26\n",
      "it great have wait NUM of god aw if you are perfect if you Likelihood = 2.7828941262874444e-28\n",
      "in opinion but onc a good wine is a paella parti as the drive Likelihood = 1.4775441610135488e-32\n",
      "environ my favorit new orlean cochon is amaz love tapa and NUM buck down Likelihood = 5.2218484929723645e-30\n",
      "don't even a spectacular i one of hard ani better meal the drink of Likelihood = 3.3632869024778396e-31\n",
      "NUM parti made in the appeal yelp with pistachio gelato a tomato NUM big Likelihood = 3.919684017838289e-37\n",
      "sauc cut his suggest this place our amaz awesom they were also had the Likelihood = 6.922540786929215e-30\n",
      "pork was becaus we could be split some store in the cocktail to write Likelihood = 7.664208524402236e-31\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    generate_sentence(interpolated_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is veri differ food drink bug me but they were too small plate and Likelihood = 4.3120835313598786e-26\n",
      "realli it amaz they were amaz creol parti includ i'm either way to compar Likelihood = 1.2545650256652837e-35\n",
      "qualiti my mouth feel like a weekend travel the citi and repres parti was Likelihood = 2.9291606930661496e-34\n",
      "a cream and belli sandwich and that has been i'v never give gaslight with Likelihood = 3.744468443219915e-30\n",
      "by et frite belong in the malnati anoth meal i'v been my biggest fan Likelihood = 2.988124473330489e-29\n",
      "portillo would rate it the line wasn't even if they like gnocchi piccolo rosa Likelihood = 4.558448340554588e-31\n",
      "hollandais sauc a group i didn't miss it definit my eye french toast better Likelihood = 2.58586891342268e-28\n",
      "and famili style veri pleasant meati all the price were abl to read all Likelihood = 6.476264218358126e-27\n",
      "i check and then go earlier than it becaus it everytim i'v been and Likelihood = 6.820637218413802e-29\n",
      "into a long to coop the food and also a for the food on Likelihood = 1.621543275230831e-27\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    generate_sentence(discounted_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what must in so-so as menu a made it that but favorit which littl Likelihood = 6.741429514013087e-37\n",
      "plate come we osso sometim the not but same liquor that oyster authent so Likelihood = 2.4618580377269585e-42\n",
      "put meal and be even my this and this they that complaint but toro Likelihood = 4.63954097654135e-35\n",
      "delici here across plain are mistaken grapefruit with bread bar still just had absolut Likelihood = 1.0450748383543077e-44\n",
      "my piec is piec the won if if of someon say between i isn't Likelihood = 2.293049248784823e-39\n",
      "these keep we salad and all oyster we a and interest the everi qualiti Likelihood = 3.032760926340397e-36\n",
      "charm the a go they we servic usual overal amaz lunch though local chicago Likelihood = 4.6626886098774336e-39\n",
      "greet sure the but someon tell had so on bore the actual walk servic Likelihood = 2.655758338362878e-39\n",
      "are buddha good the super heard but see crispi impecc fri i burger and Likelihood = 6.060858835357597e-39\n",
      "how with was just one saturday creativ i dinner the enough would ever un-heard Likelihood = 4.199386547044231e-41\n"
     ]
    }
   ],
   "source": [
    "words = list(unigram_model.keys())\n",
    "probs = list(unigram_model.values())\n",
    "for k in range(10):\n",
    "    sentence = []\n",
    "    prob = 1\n",
    "    for m in range(14):\n",
    "        p = np.random.uniform(0.0, 1.0, 1)[0]\n",
    "        j = 0\n",
    "        i = 0\n",
    "        while j < p:\n",
    "            j += probs[i]\n",
    "            i += 1\n",
    "        sentence.append(words[i-1])\n",
    "        prob = prob * probs[i-1]\n",
    "    print(' '.join(sentence), \"Likelihood = \" + str(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_reviews = cleandoc(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth the unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = [token for document in reviews for token in document]\n",
    "m = len(tokens)\n",
    "smooth_unigram = Counter(tokens)\n",
    "delta = 0.1\n",
    "for word in smooth_unigram.keys():\n",
    "    smooth_unigram[word] = (smooth_unigram[word] + delta) / (m + delta * V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UNK = delta / (m + delta * V)\n",
    "smooth_unigram['UNK'] = UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interpolated_bigrams = bigrams\n",
    "lmbda = 0.9\n",
    "for key in interpolated_bigrams.keys():\n",
    "    preceding, following = key[0], key[1]\n",
    "    interpolated_bigrams[key] = lmbda * (bigrams[key] / unigrams[preceding]) + (1.0 - lmbda) * smooth_unigram[following]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = smooth_unigram.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in vocabulary:\n",
    "    interpolated_bigrams[('UNK', word)] = (1.0 - lmbda) * smooth_unigram[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_reviews = [token for document in test_reviews for token in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigram = test_reviews[0]\n",
    "bigrams = [(a, b) for a, b in zip(test_reviews[0:-1], test_reviews[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49099181.534661114"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q =  [smooth_unigram[test_reviews[0]]]\n",
    "for ungram in test_reviews[1:]:\n",
    "    p = smooth_unigram[bigram]\n",
    "    if p > 0:\n",
    "        q.append(p)\n",
    "    else:\n",
    "        q.append(smooth_unigram['UNK'])\n",
    "perplexity = np.exp(np.mean(np.log(1.0 / np.array(q))))\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179.56371513905106"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q =  [unigram_model[test_reviews[0]]]\n",
    "for bigram in [(a, b) for a, b in zip(test_reviews[0:-1], test_reviews[1:])]:\n",
    "    p = interpolated_bigrams[bigram]\n",
    "\n",
    "    if p > 0:\n",
    "        q.append(p)\n",
    "    \n",
    "    else:\n",
    "        if bigram[1] in vocabulary:\n",
    "            q.append((1.0 - lmbda) * smooth_unigram[bigram[1]])\n",
    "        else:\n",
    "            q.append((1.0 - lmbda) * smooth_unigram['UNK'])\n",
    "perplexity = np.exp(np.mean(np.log(1.0 / np.array(q))))\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174.53965141226629"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q =  [unigram_model[test_reviews[0]]]\n",
    "for bigram in [(a, b) for a, b in zip(test_reviews[0:-1], test_reviews[1:])]:\n",
    "    p = discounted_bigrams[bigram]\n",
    "\n",
    "    if p > 0:\n",
    "        q.append(p)\n",
    "    \n",
    "    else:\n",
    "        if bigram[1] in vocabulary:\n",
    "            q.append((1.0 - lmbda) * smooth_unigram[bigram[1]])\n",
    "        else:\n",
    "            q.append((1.0 - lmbda) * smooth_unigram['UNK'])\n",
    "perplexity = np.exp(np.mean(np.log(1.0 / np.array(q))))\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179.56371514\n"
     ]
    }
   ],
   "source": [
    "perplexity = np.exp(likelihood / N)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from decimal import *\n",
    "getcontext().prec = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = np.array(likelihood, dtype=np.longfloat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = [Decimal(a) for a in likelihood]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = Decimal(1)\n",
    "for b in c:\n",
    "    a = a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = sum([1.0 / np.log(a) for a in likelihood])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72876658436251551"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(c / len(likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolated_bigrams[('honeydew', 'melon')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38241.182888818206"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(interpolated_bigrams.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('main', 'lobster'),\n",
       " ('lobster', 'roll'),\n",
       " ('roll', 'some'),\n",
       " ('some', 'of'),\n",
       " ('of', 'my'),\n",
       " ('my', 'yelp'),\n",
       " ('yelp', 'friend'),\n",
       " ('friend', 'had'),\n",
       " ('had', 'come'),\n",
       " ('come', 'here'),\n",
       " ('here', 'for'),\n",
       " ('for', 'lobster'),\n",
       " ('lobster', 'roll'),\n",
       " ('roll', 'and'),\n",
       " ('and', 'suggest'),\n",
       " ('suggest', 'that'),\n",
       " ('that', 'there'),\n",
       " ('there', 'might'),\n",
       " ('might', 'be'),\n",
       " ('be', 'long'),\n",
       " ('long', 'line'),\n",
       " ('line', 'so'),\n",
       " ('so', 'i'),\n",
       " ('i', 'just'),\n",
       " ('just', 'decid'),\n",
       " ('decid', 'to'),\n",
       " ('to', 'come'),\n",
       " ('come', 'a'),\n",
       " ('a', 'bit'),\n",
       " ('bit', 'later'),\n",
       " ('later', 'around'),\n",
       " ('around', 'NUM'),\n",
       " ('NUM', 'i'),\n",
       " ('i', 'think'),\n",
       " ('think', 'i'),\n",
       " ('i', 'still'),\n",
       " ('still', 'had'),\n",
       " ('had', 'to'),\n",
       " ('to', 'wait'),\n",
       " ('wait', 'for'),\n",
       " ('for', 'about'),\n",
       " ('about', 'NUM'),\n",
       " ('NUM', 'min'),\n",
       " ('min', 'but'),\n",
       " ('but', 'sinc'),\n",
       " ('sinc', 'i'),\n",
       " ('i', 'was'),\n",
       " ('was', 'by'),\n",
       " ('by', 'myself'),\n",
       " ('myself', 'there'),\n",
       " ('there', 'was'),\n",
       " ('was', 'a'),\n",
       " ('a', 'seat'),\n",
       " ('seat', 'at'),\n",
       " ('at', 'the'),\n",
       " ('the', 'bar'),\n",
       " ('bar', 'so'),\n",
       " ('so', 'i'),\n",
       " ('i', 'took'),\n",
       " ('took', 'it'),\n",
       " ('it', 'i'),\n",
       " ('i', 'got'),\n",
       " ('got', 'the'),\n",
       " ('the', 'warm'),\n",
       " ('warm', 'lobster'),\n",
       " ('lobster', 'roll'),\n",
       " ('roll', 'with'),\n",
       " ('with', 'some'),\n",
       " ('some', 'french'),\n",
       " ('french', 'fri'),\n",
       " ('fri', 'french'),\n",
       " ('french', 'fri'),\n",
       " ('fri', 'were'),\n",
       " ('were', 'averag'),\n",
       " ('averag', 'not'),\n",
       " ('not', 'bad'),\n",
       " ('bad', 'but'),\n",
       " ('but', 'the'),\n",
       " ('the', 'lobster'),\n",
       " ('lobster', 'roll'),\n",
       " ('roll', 'omg'),\n",
       " ('omg', 'it'),\n",
       " ('it', 'was'),\n",
       " ('was', 'fill'),\n",
       " ('fill', 'to'),\n",
       " ('to', 'the'),\n",
       " ('the', 'brim'),\n",
       " ('brim', 'and'),\n",
       " ('and', 'there'),\n",
       " ('there', 'was'),\n",
       " ('was', 'actual'),\n",
       " ('actual', 'so'),\n",
       " ('so', 'much'),\n",
       " ('much', 'lobster'),\n",
       " ('lobster', 'it'),\n",
       " ('it', 'was'),\n",
       " ('was', 'hard'),\n",
       " ('hard', 'to'),\n",
       " ('to', 'eat'),\n",
       " ('eat', 'i'),\n",
       " ('i', 'had'),\n",
       " ('had', 'to'),\n",
       " ('to', 'cut'),\n",
       " ('cut', 'it'),\n",
       " ('it', 'in'),\n",
       " ('in', 'half'),\n",
       " ('half', 'to'),\n",
       " ('to', 'eat'),\n",
       " ('eat', 'it'),\n",
       " ('it', 'appropri'),\n",
       " ('appropri', 'nice'),\n",
       " ('nice', 'and'),\n",
       " ('and', 'butteri'),\n",
       " ('butteri', 'too'),\n",
       " ('too', 'i'),\n",
       " ('i', 'realli'),\n",
       " ('realli', 'wish'),\n",
       " ('wish', 'i'),\n",
       " ('i', 'had'),\n",
       " ('had', 'more'),\n",
       " ('more', 'room'),\n",
       " ('room', 'but'),\n",
       " ('but', 'there'),\n",
       " ('there', 'look'),\n",
       " ('look', 'like'),\n",
       " ('like', 'there'),\n",
       " ('there', 'was'),\n",
       " ('was', 'some'),\n",
       " ('some', 'other'),\n",
       " ('other', 'amaz'),\n",
       " ('amaz', 'thing'),\n",
       " ('thing', 'on'),\n",
       " ('on', 'the'),\n",
       " ('the', 'menu'),\n",
       " ('menu', 'definit'),\n",
       " ('definit', 'the'),\n",
       " ('the', 'lobster'),\n",
       " ('lobster', 'roll'),\n",
       " ('roll', 'was'),\n",
       " ('was', 'a'),\n",
       " ('a', 'great'),\n",
       " ('great', 'choic'),\n",
       " ('choic', 'i'),\n",
       " ('i', 'was'),\n",
       " ('was', 'crave'),\n",
       " ('crave', 'a'),\n",
       " ('a', 'lobster'),\n",
       " ('lobster', 'roll'),\n",
       " ('roll', 'while'),\n",
       " ('while', 'visit'),\n",
       " ('visit', 'new'),\n",
       " ('new', 'england'),\n",
       " ('england', 'and'),\n",
       " ('and', 'was'),\n",
       " ('was', 'reccommend'),\n",
       " ('reccommend', 'this'),\n",
       " ('this', 'by'),\n",
       " ('by', 'a'),\n",
       " ('a', 'friend'),\n",
       " ('friend', 'much'),\n",
       " ('much', 'like'),\n",
       " ('like', 'the'),\n",
       " ('the', 'rest'),\n",
       " ('rest', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'restur'),\n",
       " ('restur', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'north'),\n",
       " ('north', 'end'),\n",
       " ('end', 'this'),\n",
       " ('this', 'place'),\n",
       " ('place', 'had'),\n",
       " ('had', 'a'),\n",
       " ('a', 'wait'),\n",
       " ('wait', 'and'),\n",
       " ('and', 'you'),\n",
       " ('you', 'will'),\n",
       " ('will', 'most'),\n",
       " ('most', 'like'),\n",
       " ('like', 'share'),\n",
       " ('share', 'a'),\n",
       " ('a', 'tabl'),\n",
       " ('tabl', 'with'),\n",
       " ('with', 'other'),\n",
       " ('other', 'howev'),\n",
       " ('howev', 'the'),\n",
       " ('the', 'food'),\n",
       " ('food', 'make'),\n",
       " ('make', 'it'),\n",
       " ('it', 'worth'),\n",
       " ('worth', 'it'),\n",
       " ('it', 'i'),\n",
       " ('i', 'had'),\n",
       " ('had', 'the'),\n",
       " ('the', 'hot'),\n",
       " ('hot', 'with'),\n",
       " ('with', 'butter'),\n",
       " ('butter', 'lobster'),\n",
       " ('lobster', 'roll'),\n",
       " ('roll', 'and'),\n",
       " ('and', 'a'),\n",
       " ('a', 'bowl'),\n",
       " ('bowl', 'of'),\n",
       " ('of', 'their'),\n",
       " ('their', 'famous'),\n",
       " ('famous', 'clam'),\n",
       " ('clam', 'chowder'),\n",
       " ('chowder', 'while'),\n",
       " ('while', 'the'),\n",
       " ('the', 'chowder'),\n",
       " ('chowder', 'was'),\n",
       " ('was', 'good'),\n",
       " ('good', 'it'),\n",
       " ('it', \"isn't\"),\n",
       " (\"isn't\", 'whi'),\n",
       " ('whi', 'this'),\n",
       " ('this', 'place'),\n",
       " ('place', 'receiv'),\n",
       " ('receiv', 'a'),\n",
       " ('a', 'NUM'),\n",
       " ('NUM', 'star'),\n",
       " ('star', 'rate'),\n",
       " ('rate', 'person'),\n",
       " ('person', \"i'v\"),\n",
       " (\"i'v\", 'never'),\n",
       " ('never', 'been'),\n",
       " ('been', 'a'),\n",
       " ('a', 'fan'),\n",
       " ('fan', 'of'),\n",
       " ('of', 'lobster'),\n",
       " ('lobster', 'but'),\n",
       " ('but', 'i'),\n",
       " ('i', 'thought'),\n",
       " ('thought', 'i'),\n",
       " ('i', 'should'),\n",
       " ('should', 'give'),\n",
       " ('give', 'it'),\n",
       " ('it', 'a'),\n",
       " ('a', 'tri'),\n",
       " ('tri', 'while'),\n",
       " ('while', 'in'),\n",
       " ('in', 'boston'),\n",
       " ('boston', 'the'),\n",
       " ('the', 'roll'),\n",
       " ('roll', 'was'),\n",
       " ('was', 'spot'),\n",
       " ('spot', 'on'),\n",
       " ('on', 'not'),\n",
       " ('not', 'too'),\n",
       " ('too', 'much'),\n",
       " ('much', 'butter'),\n",
       " ('butter', 'but'),\n",
       " ('but', 'enough'),\n",
       " ('enough', 'to'),\n",
       " ('to', 'notic'),\n",
       " ('notic', 'the'),\n",
       " ('the', 'lobster'),\n",
       " ('lobster', 'was'),\n",
       " ('was', 'cook'),\n",
       " ('cook', 'to'),\n",
       " ('to', 'perfect'),\n",
       " ('perfect', 'overpr'),\n",
       " ('overpr', 'and'),\n",
       " ('and', 'over-hyp'),\n",
       " ('over-hyp', 'lobster'),\n",
       " ('lobster', 'roll'),\n",
       " ('roll', 'though'),\n",
       " ('though', 'generous'),\n",
       " ('generous', 'size'),\n",
       " ('size', 'the'),\n",
       " ('the', 'roll'),\n",
       " ('roll', \"wasn't\"),\n",
       " (\"wasn't\", 'at'),\n",
       " ('at', 'all'),\n",
       " ('all', 'impress'),\n",
       " ('impress', 'despit'),\n",
       " ('despit', 'the'),\n",
       " ('the', 'reput'),\n",
       " ('reput', 'that'),\n",
       " ('that', 'proceed'),\n",
       " ('proceed', 'it'),\n",
       " ('it', 'bland'),\n",
       " ('bland', 'lobster'),\n",
       " ('lobster', 'though'),\n",
       " ('though', 'it'),\n",
       " ('it', 'had'),\n",
       " ('had', 'a'),\n",
       " ('a', 'decent'),\n",
       " ('decent', 'textur'),\n",
       " ('textur', 'midway'),\n",
       " ('midway', 'through'),\n",
       " ('through', 'eat'),\n",
       " ('eat', 'the'),\n",
       " ('the', 'hot'),\n",
       " ('hot', 'butter'),\n",
       " ('butter', 'lobster'),\n",
       " ('lobster', 'roll'),\n",
       " ('roll', 'i'),\n",
       " ('i', 'notic'),\n",
       " ('notic', 'a'),\n",
       " ('a', 'flavor'),\n",
       " ('flavor', 'not'),\n",
       " ('not', 'unlik'),\n",
       " ('unlik', 'an'),\n",
       " ('an', 'unclean'),\n",
       " ('unclean', 'shrimp'),\n",
       " ('shrimp', 'there'),\n",
       " ('there', 'was'),\n",
       " ('was', 'definit'),\n",
       " ('definit', 'the'),\n",
       " ('the', 'bitter'),\n",
       " ('bitter', 'tast'),\n",
       " ('tast', 'of'),\n",
       " ('of', 'crustacean'),\n",
       " ('crustacean', 'fecal'),\n",
       " ('fecal', 'matter'),\n",
       " ('matter', 'i'),\n",
       " ('i', \"wasn't\"),\n",
       " (\"wasn't\", 'use'),\n",
       " ('use', 'to'),\n",
       " ('to', 'when'),\n",
       " ('when', 'eat'),\n",
       " ('eat', 'lobster'),\n",
       " ('lobster', 'toward'),\n",
       " ('toward', 'the'),\n",
       " ('the', 'end'),\n",
       " ('end', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'roll'),\n",
       " ('roll', 'i'),\n",
       " ('i', 'was'),\n",
       " ('was', 'hit'),\n",
       " ('hit', 'with'),\n",
       " ('with', 'a'),\n",
       " ('a', 'strong'),\n",
       " ('strong', 'blast'),\n",
       " ('blast', 'of'),\n",
       " ('of', 'black'),\n",
       " ('black', 'pepper'),\n",
       " ('pepper', 'look'),\n",
       " ('look', 'like'),\n",
       " ('like', 'it'),\n",
       " ('it', \"wasn't\"),\n",
       " (\"wasn't\", 'even'),\n",
       " ('even', 'season'),\n",
       " ('season', 'sinc'),\n",
       " ('sinc', 'i'),\n",
       " ('i', \"hadn't\"),\n",
       " (\"hadn't\", 'tast'),\n",
       " ('tast', 'a'),\n",
       " ('a', 'hint'),\n",
       " ('hint', 'of'),\n",
       " ('of', 'that'),\n",
       " ('that', 'pepper'),\n",
       " ('pepper', 'through'),\n",
       " ('through', 'NUM'),\n",
       " ('NUM', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'thing'),\n",
       " ('thing', 'i'),\n",
       " ('i', 'recommend'),\n",
       " ('recommend', 'you'),\n",
       " ('you', 'ask'),\n",
       " ('ask', 'for'),\n",
       " ('for', 'a'),\n",
       " ('a', 'wedg'),\n",
       " ('wedg', 'of'),\n",
       " ('of', 'lemon'),\n",
       " ('lemon', 'if'),\n",
       " ('if', 'you'),\n",
       " ('you', 'decid'),\n",
       " ('decid', 'to'),\n",
       " ('to', 'actual'),\n",
       " ('actual', 'tri'),\n",
       " ('tri', 'this'),\n",
       " ('this', 'entr'),\n",
       " ('entr', 'é'),\n",
       " ('é', 'for'),\n",
       " ('for', 'yourself'),\n",
       " ('yourself', 'my'),\n",
       " ('my', 'friend'),\n",
       " ('friend', 'who'),\n",
       " ('who', 'had'),\n",
       " ('had', 'the'),\n",
       " ('the', 'roll'),\n",
       " ('roll', 'with'),\n",
       " ('with', 'mayo'),\n",
       " ('mayo', 'remark'),\n",
       " ('remark', 'that'),\n",
       " ('that', 'the'),\n",
       " ('the', 'mayo'),\n",
       " ('mayo', 'flavor'),\n",
       " ('flavor', 'was'),\n",
       " ('was', 'bare'),\n",
       " ('bare', 'notic'),\n",
       " ('notic', 'the'),\n",
       " ('the', 'fri'),\n",
       " ('fri', 'that'),\n",
       " ('that', 'came'),\n",
       " ('came', 'with'),\n",
       " ('with', 'the'),\n",
       " ('the', 'dish'),\n",
       " ('dish', 'were'),\n",
       " ('were', 'forgett'),\n",
       " ('forgett', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'end'),\n",
       " ('end', 'it'),\n",
       " ('it', 'came'),\n",
       " ('came', 'down'),\n",
       " ('down', 'to'),\n",
       " ('to', '$'),\n",
       " ('$', 'NUM'),\n",
       " ('NUM', 'for'),\n",
       " ('for', 'NUM'),\n",
       " ('NUM', 'oyster'),\n",
       " ('oyster', 'shooter'),\n",
       " ('shooter', 'and'),\n",
       " ('and', 'roll'),\n",
       " ('roll', 'plus'),\n",
       " ('plus', 'tip'),\n",
       " ('tip', '$'),\n",
       " ('$', 'NUM'),\n",
       " ('NUM', 'for'),\n",
       " ('for', 'the'),\n",
       " ('the', 'roll'),\n",
       " ('roll', 'and'),\n",
       " ('and', '$'),\n",
       " ('$', '2'),\n",
       " ('2', '60-'),\n",
       " ('60-', 'NUM'),\n",
       " ('NUM', 'for'),\n",
       " ('for', 'the'),\n",
       " ('the', 'shooter'),\n",
       " ('shooter', 'the'),\n",
       " ('the', 'winter'),\n",
       " ('winter', 'was'),\n",
       " ('was', 'my'),\n",
       " ('my', 'favorit'),\n",
       " ('favorit', 'not'),\n",
       " ('not', 'be'),\n",
       " ('be', 'too'),\n",
       " ('too', 'fishi'),\n",
       " ('fishi', 'with'),\n",
       " ('with', 'a'),\n",
       " ('a', 'meati'),\n",
       " ('meati', 'textur'),\n",
       " ('textur', 'the'),\n",
       " ('the', 'kumamoto'),\n",
       " ('kumamoto', 'tast'),\n",
       " ('tast', 'like'),\n",
       " ('like', 'eat'),\n",
       " ('eat', 'a'),\n",
       " ('a', 'honeydew'),\n",
       " ('honeydew', 'melon'),\n",
       " ('melon', 'by'),\n",
       " ('by', 'the'),\n",
       " ('the', 'sea'),\n",
       " ('sea', 'it'),\n",
       " ('it', 'had'),\n",
       " ('had', 'a'),\n",
       " ('a', 'strong'),\n",
       " ('strong', 'ocean'),\n",
       " ('ocean', 'flavor'),\n",
       " ('flavor', 'with'),\n",
       " ('with', 'a'),\n",
       " ('a', 'hint'),\n",
       " ('hint', 'of'),\n",
       " ('of', 'melon'),\n",
       " ('melon', 'it'),\n",
       " ('it', 'had'),\n",
       " ('had', 'me'),\n",
       " ('me', 'wax'),\n",
       " ('wax', 'nostalg'),\n",
       " ('nostalg', 'sinc'),\n",
       " ('sinc', 'this'),\n",
       " ('this', 'particular'),\n",
       " ('particular', 'oyster'),\n",
       " ('oyster', 'origin'),\n",
       " ('origin', 'from'),\n",
       " ('from', 'the'),\n",
       " ('the', 'veri'),\n",
       " ('veri', 'citi'),\n",
       " ('citi', 'i'),\n",
       " ('i', 'had'),\n",
       " ('had', 'live'),\n",
       " ('live', 'for'),\n",
       " ('for', 'year'),\n",
       " ('year', 'in'),\n",
       " ('in', 'japan'),\n",
       " ('japan', 'as'),\n",
       " ('as', 'for'),\n",
       " ('for', 'the'),\n",
       " ('the', 'servic'),\n",
       " ('servic', 'it'),\n",
       " ('it', 'was'),\n",
       " ('was', 'decent'),\n",
       " ('decent', 'though'),\n",
       " ('though', 'a'),\n",
       " ('a', 'bit'),\n",
       " ('bit', 'bursqu'),\n",
       " ('bursqu', 'and'),\n",
       " ('and', 'a'),\n",
       " ('a', 'bit'),\n",
       " ('bit', 'bizarr'),\n",
       " ('bizarr', 'the'),\n",
       " ('the', 'staff'),\n",
       " ('staff', 'talk'),\n",
       " ('talk', 'amongst'),\n",
       " ('amongst', 'each'),\n",
       " ('each', 'other'),\n",
       " ('other', 'about'),\n",
       " ('about', 'the'),\n",
       " ('the', 'custom'),\n",
       " ('custom', 'as'),\n",
       " ('as', 'though'),\n",
       " ('though', 'we'),\n",
       " ('we', \"weren't\"),\n",
       " (\"weren't\", 'right'),\n",
       " ('right', 'there'),\n",
       " ('there', 'get'),\n",
       " ('get', 'your'),\n",
       " ('your', 'name'),\n",
       " ('name', 'on'),\n",
       " ('on', 'that'),\n",
       " ('that', 'list'),\n",
       " ('list', 'it'),\n",
       " ('it', 'total'),\n",
       " ('total', 'worth'),\n",
       " ('worth', 'it'),\n",
       " ('it', 'in'),\n",
       " ('in', 'houston'),\n",
       " ('houston', 'there'),\n",
       " ('there', 'realli'),\n",
       " ('realli', 'no'),\n",
       " ('no', 'such'),\n",
       " ('such', 'thing'),\n",
       " ('thing', 'as'),\n",
       " ('as', 'wait'),\n",
       " ('wait', 'NUM'),\n",
       " ('NUM', 'hour'),\n",
       " ('hour', 'for'),\n",
       " ('for', 'dinner'),\n",
       " ('dinner', 'let'),\n",
       " ('let', 'alon'),\n",
       " ('alon', 'a'),\n",
       " ('a', 'box'),\n",
       " ('box', 'of'),\n",
       " ('of', 'pastri'),\n",
       " ('pastri', 'in'),\n",
       " ('in', 'fact'),\n",
       " ('fact', 'i'),\n",
       " ('i', \"don't\"),\n",
       " (\"don't\", 'even'),\n",
       " ('even', 'think'),\n",
       " ('think', \"i'd\"),\n",
       " (\"i'd\", 'wait'),\n",
       " ('wait', 'more'),\n",
       " ('more', 'than'),\n",
       " ('than', 'one'),\n",
       " ('one', 'hour'),\n",
       " ('hour', 'for'),\n",
       " ('for', 'food'),\n",
       " ('food', 'at'),\n",
       " ('at', 'a'),\n",
       " ('a', 'restaur'),\n",
       " ('restaur', 'i'),\n",
       " ('i', 'get'),\n",
       " ('get', 'hangri'),\n",
       " ('hangri', ');'),\n",
       " (');', 'howev'),\n",
       " ('howev', 'at'),\n",
       " ('at', 'neptun'),\n",
       " ('neptun', 'more'),\n",
       " ('more', 'often'),\n",
       " ('often', 'than'),\n",
       " ('than', 'not'),\n",
       " ('not', 'you'),\n",
       " ('you', 'will'),\n",
       " ('will', 'have'),\n",
       " ('have', 'to'),\n",
       " ('to', 'give'),\n",
       " ('give', 'your'),\n",
       " ('your', 'name'),\n",
       " ('name', 'and'),\n",
       " ('and', 'phone'),\n",
       " ('phone', 'number'),\n",
       " ('number', 'to'),\n",
       " ('to', 'the'),\n",
       " ('the', 'host'),\n",
       " ('host', 'hostess'),\n",
       " ('hostess', 'if'),\n",
       " ('if', 'you'),\n",
       " ('you', 'want'),\n",
       " ('want', 'to'),\n",
       " ('to', 'have'),\n",
       " ('have', 'dinner'),\n",
       " ('dinner', 'at'),\n",
       " ('at', 'this'),\n",
       " ('this', 'restaur'),\n",
       " ('restaur', \"i'd\"),\n",
       " (\"i'd\", 'recommend'),\n",
       " ('recommend', 'put'),\n",
       " ('put', 'your'),\n",
       " ('your', 'name'),\n",
       " ('name', 'on'),\n",
       " ('on', 'the'),\n",
       " ('the', 'list'),\n",
       " ('list', 'around'),\n",
       " ('around', 'TIME'),\n",
       " ('TIME', 'if'),\n",
       " ('if', \"you'r\"),\n",
       " (\"you'r\", 'plan'),\n",
       " ('plan', 'to'),\n",
       " ('to', 'eat'),\n",
       " ('eat', 'dinner'),\n",
       " ('dinner', 'around'),\n",
       " ('around', 'NUM'),\n",
       " ('NUM', 'or'),\n",
       " ('or', '8p'),\n",
       " ('8p', 'm'),\n",
       " ('m', 'my'),\n",
       " ('my', 'boyfriend'),\n",
       " ('boyfriend', 'and'),\n",
       " ('and', 'i'),\n",
       " ('i', 'put'),\n",
       " ('put', 'our'),\n",
       " ('our', 'name'),\n",
       " ('name', 'on'),\n",
       " ('on', 'the'),\n",
       " ('the', 'list'),\n",
       " ('list', 'at'),\n",
       " ('at', 'TIME'),\n",
       " ('TIME', 'on'),\n",
       " ('on', 'a'),\n",
       " ('a', 'saturday'),\n",
       " ('saturday', 'night'),\n",
       " ('night', 'and'),\n",
       " ('and', 'we'),\n",
       " ('we', 'were'),\n",
       " ('were', 'told'),\n",
       " ('told', 'that'),\n",
       " ('that', 'it'),\n",
       " ('it', 'would'),\n",
       " ('would', 'be'),\n",
       " ('be', 'a'),\n",
       " ('a', 'NUM'),\n",
       " ('NUM', 'hour'),\n",
       " ('hour', 'wait'),\n",
       " ('wait', 'be'),\n",
       " ('be', 'tourist'),\n",
       " ('tourist', 'we'),\n",
       " ('we', 'were'),\n",
       " ('were', 'perfect'),\n",
       " ('perfect', 'fine'),\n",
       " ('fine', 'with'),\n",
       " ('with', 'dinner'),\n",
       " ('dinner', 'around'),\n",
       " ('around', '8p'),\n",
       " ('8p', 'm'),\n",
       " ('m', 'in'),\n",
       " ('in', 'fact'),\n",
       " ('fact', 'we'),\n",
       " ('we', 'took'),\n",
       " ('took', 'this'),\n",
       " ('this', 'extra'),\n",
       " ('extra', 'time'),\n",
       " ('time', 'to'),\n",
       " ('to', 'explor'),\n",
       " ('explor', 'and'),\n",
       " ('and', 'had'),\n",
       " ('had', 'dessert'),\n",
       " ('dessert', 'befor'),\n",
       " ('befor', 'dinner'),\n",
       " ('dinner', 'at'),\n",
       " ('at', 'one'),\n",
       " ('one', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'local'),\n",
       " ('local', 'pastri'),\n",
       " ('pastri', 'shop'),\n",
       " ('shop', 'nearbi'),\n",
       " ('nearbi', 'around'),\n",
       " ('around', 'TIME'),\n",
       " ('TIME', 'i'),\n",
       " ('i', 'decid'),\n",
       " ('decid', 'to'),\n",
       " ('to', 'look'),\n",
       " ('look', 'at'),\n",
       " ('at', 'my'),\n",
       " ('my', 'phone'),\n",
       " ('phone', 'to'),\n",
       " ('to', 'see'),\n",
       " ('see', 'what'),\n",
       " ('what', 'time'),\n",
       " ('time', 'it'),\n",
       " ('it', 'was'),\n",
       " ('was', 'that'),\n",
       " ('that', 'when'),\n",
       " ('when', 'i'),\n",
       " ('i', 'realiz'),\n",
       " ('realiz', 'that'),\n",
       " ('that', 'i'),\n",
       " ('i', 'had'),\n",
       " ('had', 'two'),\n",
       " ('two', 'miss'),\n",
       " ('miss', 'call'),\n",
       " ('call', 'from'),\n",
       " ('from', 'neptun'),\n",
       " ('neptun', 'appar'),\n",
       " ('appar', 'my'),\n",
       " ('my', 'phone'),\n",
       " ('phone', 'was'),\n",
       " ('was', 'on'),\n",
       " ('on', 'silent'),\n",
       " ('silent', 'luckili'),\n",
       " ('luckili', 'while'),\n",
       " ('while', 'i'),\n",
       " ('i', 'was'),\n",
       " ('was', 'stare'),\n",
       " ('stare', 'at'),\n",
       " ('at', 'my'),\n",
       " ('my', 'phone'),\n",
       " ('phone', 'the'),\n",
       " ('the', 'host'),\n",
       " ('host', 'from'),\n",
       " ('from', 'neptun'),\n",
       " ('neptun', 'call'),\n",
       " ('call', 'for'),\n",
       " ('for', 'a'),\n",
       " ('a', 'NUM'),\n",
       " ('NUM', 'attempt'),\n",
       " ('attempt', 'NUM'),\n",
       " ('NUM', 'time'),\n",
       " ('time', 'a'),\n",
       " ('a', 'charm'),\n",
       " ('charm', 'and'),\n",
       " ('and', 'i'),\n",
       " ('i', 'pick'),\n",
       " ('pick', 'up'),\n",
       " ('up', 'right'),\n",
       " ('right', 'away'),\n",
       " ('away', 'lesson'),\n",
       " ('lesson', 'to'),\n",
       " ('to', 'be'),\n",
       " ('be', 'learn'),\n",
       " ('learn', 'after'),\n",
       " ('after', 'you'),\n",
       " ('you', 'put'),\n",
       " ('put', 'you'),\n",
       " ('you', 'name'),\n",
       " ('name', 'on'),\n",
       " ('on', 'the'),\n",
       " ('the', 'list'),\n",
       " ('list', 'at'),\n",
       " ('at', 'neptun'),\n",
       " ('neptun', 'make'),\n",
       " ('make', 'sure'),\n",
       " ('sure', 'the'),\n",
       " ('the', 'ringer'),\n",
       " ('ringer', 'volum'),\n",
       " ('volum', 'on'),\n",
       " ('on', 'your'),\n",
       " ('your', 'phone'),\n",
       " ('phone', 'is'),\n",
       " ('is', 'turn'),\n",
       " ('turn', 'on'),\n",
       " ('on', 'after'),\n",
       " ('after', 'hear'),\n",
       " ('hear', 'that'),\n",
       " ('that', 'we'),\n",
       " ('we', \"didn't\"),\n",
       " (\"didn't\", 'have'),\n",
       " ('have', 'to'),\n",
       " ('to', 'wait'),\n",
       " ('wait', 'an'),\n",
       " ('an', 'extra'),\n",
       " ('extra', 'hour'),\n",
       " ('hour', 'to'),\n",
       " ('to', 'be'),\n",
       " ('be', 'seat'),\n",
       " ('seat', 'my'),\n",
       " ('my', 'boyfriend'),\n",
       " ('boyfriend', 'and'),\n",
       " ('and', 'i'),\n",
       " ('i', 'walk'),\n",
       " ('walk', 'to'),\n",
       " ('to', 'neptun'),\n",
       " ('neptun', 'and'),\n",
       " ('and', 'we'),\n",
       " ('we', 'were'),\n",
       " ('were', 'seat'),\n",
       " ('seat', 'at'),\n",
       " ('at', 'the'),\n",
       " ('the', 'bar'),\n",
       " ('bar', 'within'),\n",
       " ('within', 'NUM'),\n",
       " ('NUM', 'minut'),\n",
       " ('minut', 'for'),\n",
       " ('for', 'starter'),\n",
       " ('starter', 'we'),\n",
       " ('we', 'order'),\n",
       " ('order', 'a'),\n",
       " ('a', 'few'),\n",
       " ('few', 'differ'),\n",
       " ('differ', 'kind'),\n",
       " ('kind', 'of'),\n",
       " ('of', 'raw'),\n",
       " ('raw', 'oyster'),\n",
       " ('oyster', 'from'),\n",
       " ('from', 'their'),\n",
       " ('their', 'terrif'),\n",
       " ('terrif', 'select'),\n",
       " ('select', 'i'),\n",
       " ('i', 'wish'),\n",
       " ('wish', 'we'),\n",
       " ('we', 'could'),\n",
       " ('could', 'have'),\n",
       " ('have', 'tri'),\n",
       " ('tri', 'everi'),\n",
       " ('everi', 'singl'),\n",
       " ('singl', 'type'),\n",
       " ('type', 'but'),\n",
       " ('but', 'we'),\n",
       " ('we', 'might'),\n",
       " ('might', 'have'),\n",
       " ('have', 'spoil'),\n",
       " ('spoil', 'our'),\n",
       " ('our', 'appetit'),\n",
       " ('appetit', 'just'),\n",
       " ('just', 'a'),\n",
       " ('a', 'tad'),\n",
       " ('tad', 'by'),\n",
       " ('by', 'eat'),\n",
       " ('eat', 'dessert'),\n",
       " ('dessert', 'befor'),\n",
       " ('befor', 'dinner'),\n",
       " ('dinner', 'oop'),\n",
       " ('oop', 'we'),\n",
       " ('we', 'order'),\n",
       " ('order', 'NUM'),\n",
       " ('NUM', 'wellfleet'),\n",
       " ('wellfleet', 'NUM'),\n",
       " ('NUM', 'pleasant'),\n",
       " ('pleasant', 'bay'),\n",
       " ('bay', 'NUM'),\n",
       " ('NUM', 'summersid'),\n",
       " ('summersid', 'NUM'),\n",
       " ('NUM', 'beausoleil'),\n",
       " ('beausoleil', 'NUM'),\n",
       " ('NUM', 'kumamoto'),\n",
       " ('kumamoto', 'and'),\n",
       " ('and', 'NUM'),\n",
       " ('NUM', 'jonah'),\n",
       " ('jonah', 'crab'),\n",
       " ('crab', 'claw'),\n",
       " ('claw', 'the'),\n",
       " ('the', 'crab'),\n",
       " ('crab', 'claw'),\n",
       " ('claw', \"weren't\"),\n",
       " (\"weren't\", 'anyth'),\n",
       " ('anyth', 'special'),\n",
       " ('special', 'just'),\n",
       " ('just', 'regular'),\n",
       " ('regular', 'stone'),\n",
       " ('stone', 'crab'),\n",
       " ('crab', 'claw'),\n",
       " ('claw', 'i'),\n",
       " ('i', 'probabl'),\n",
       " ('probabl', \"wouldn't\"),\n",
       " (\"wouldn't\", 'order'),\n",
       " ('order', 'them'),\n",
       " ('them', 'again'),\n",
       " ('again', 'my'),\n",
       " ('my', 'favorit'),\n",
       " ('favorit', 'oyster'),\n",
       " ('oyster', 'were'),\n",
       " ('were', 'the'),\n",
       " ('the', 'wellfleet'),\n",
       " ('wellfleet', 'and'),\n",
       " ('and', 'the'),\n",
       " ('the', 'kumamoto'),\n",
       " ('kumamoto', 'the'),\n",
       " ('the', 'wellfleet'),\n",
       " ('wellfleet', 'were'),\n",
       " ('were', 'salti'),\n",
       " ('salti', 'and'),\n",
       " ('and', 'butteri'),\n",
       " ('butteri', 'yum'),\n",
       " ('yum', 'and'),\n",
       " ('and', 'the'),\n",
       " ('the', 'kumamoto'),\n",
       " ('kumamoto', 'realli'),\n",
       " ('realli', 'did'),\n",
       " ('did', 'tast'),\n",
       " ('tast', 'like'),\n",
       " ('like', 'cold'),\n",
       " ('cold', 'creami'),\n",
       " ('creami', 'honeydew'),\n",
       " ('honeydew', 'i'),\n",
       " ('i', 'thought'),\n",
       " ('thought', 'that'),\n",
       " ('that', 'neptun'),\n",
       " ('neptun', 'descript'),\n",
       " ('descript', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'kumamoto'),\n",
       " ('kumamoto', 'was'),\n",
       " ('was', 'weird'),\n",
       " ('weird', 'but'),\n",
       " ('but', 'after'),\n",
       " ('after', 'eat'),\n",
       " ('eat', 'these'),\n",
       " ('these', 'oyster'),\n",
       " ('oyster', 'i'),\n",
       " ('i', 'could'),\n",
       " ('could', 'definit'),\n",
       " ('definit', 'tast'),\n",
       " ('tast', 'the'),\n",
       " ('the', 'sweet'),\n",
       " ('sweet', 'honeydew'),\n",
       " ('honeydew', 'flavor'),\n",
       " ('flavor', 'high'),\n",
       " ('high', 'recommend'),\n",
       " ('recommend', 'the'),\n",
       " ('the', 'kumamoto'),\n",
       " ('kumamoto', 'oyster'),\n",
       " ('oyster', 'the'),\n",
       " ('the', 'johnni'),\n",
       " ('johnni', 'cake'),\n",
       " ('cake', 'were'),\n",
       " ('were', 'our'),\n",
       " ('our', 'next'),\n",
       " ('next', 'appet'),\n",
       " ('appet', 'and'),\n",
       " ('and', 'i'),\n",
       " ('i', 'would'),\n",
       " ('would', 'also'),\n",
       " ('also', 'recommend'),\n",
       " ('recommend', 'order'),\n",
       " ('order', 'this'),\n",
       " ('this', 'it'),\n",
       " ('it', 'the'),\n",
       " ('the', 'perfect'),\n",
       " ('perfect', 'combin'),\n",
       " ('combin', 'of'),\n",
       " ('of', 'sweet'),\n",
       " ('sweet', 'and'),\n",
       " ('and', 'salti'),\n",
       " ('salti', 'the'),\n",
       " ('the', 'salti'),\n",
       " ('salti', 'portion'),\n",
       " ('portion', 'consist'),\n",
       " ('consist', 'of'),\n",
       " ('of', 'smoke'),\n",
       " ('smoke', 'trout'),\n",
       " ('trout', 'american'),\n",
       " ('american', 'black'),\n",
       " ('black', 'caviar'),\n",
       " ('caviar', 'and'),\n",
       " ('and', 'creme'),\n",
       " ('creme', 'fraich'),\n",
       " ('fraich', 'in'),\n",
       " ('in', 'a'),\n",
       " ('a', 'tower'),\n",
       " ('tower', 'on'),\n",
       " ('on', 'top'),\n",
       " ('top', 'of'),\n",
       " ('of', 'a'),\n",
       " ('a', 'larg'),\n",
       " ('larg', 'sweet'),\n",
       " ('sweet', 'corn'),\n",
       " ('corn', 'pancak'),\n",
       " ('pancak', 'spread'),\n",
       " ('spread', 'the'),\n",
       " ('the', 'smoke'),\n",
       " ('smoke', 'fish'),\n",
       " ('fish', 'caviar'),\n",
       " ('caviar', 'and'),\n",
       " ('and', 'creme'),\n",
       " ('creme', 'fraich'),\n",
       " ('fraich', 'over'),\n",
       " ('over', 'the'),\n",
       " ('the', 'entir'),\n",
       " ('entir', 'johnni'),\n",
       " ('johnni', 'cake'),\n",
       " ('cake', 'cut'),\n",
       " ('cut', 'off'),\n",
       " ('off', 'a'),\n",
       " ('a', 'littl'),\n",
       " ('littl', 'piec'),\n",
       " ('piec', 'and'),\n",
       " ('and', 'eat'),\n",
       " ('eat', 'it'),\n",
       " ('it', 'altogeth'),\n",
       " ('altogeth', 'in'),\n",
       " ('in', 'one'),\n",
       " ('one', 'bite'),\n",
       " ('bite', 'it'),\n",
       " ...]"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = set([token for review in reviews for token in review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = list(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary_to_index = {}\n",
    "for index, word in enumerate(vocabulary):\n",
    "    vocabulary_to_index[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_bigram(documents):\n",
    "    bigrams = [bigram for document in documents for bigram in zip(*[document[i:] for i in range(2)])]\n",
    "    m = len(bigrams)\n",
    "    I = np.zeros(m)\n",
    "    J = np.zeros(m)\n",
    "    D = np.ones(m)\n",
    "    for index, bigram in enumerate(bigrams): \n",
    "        I[index] = vocabulary_to_index[bigram[0]] \n",
    "        J[index] = vocabulary_to_index[bigram[1]]\n",
    "    bigram_matrix = sparse.coo_matrix((D,(I,J)),shape=(vocab_size,vocab_size)).tocsr()\n",
    "    return bigram_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = get_bigram(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def access_bigram_matrix(word1, word2, bigram_matrix):\n",
    "    return(bigram_matrix[vocabulary_to_index[word1], vocabulary_to_index[word2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigram = Counter([token for review in reviews for token in review])\n",
    "priors = np.zeros(vocab_size)\n",
    "for word in unigram.keys():\n",
    "    priors[vocabulary_to_index[word]] = unigram[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   4.,    2.,  889., ...,    1.,    1.,    1.])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "priors = np.ones(vocab_size) / priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<33726x33726 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 675716 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<33726x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 33726 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse.csr_matrix(np.reshape(priors, (vocab_size, 1)).dot(np.ones()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-279-4be627571020>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpriors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Antoine\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__truediv__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__truediv__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_divide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_divide\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__div__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Antoine\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_divide\u001b[1;34m(self, other, true_divide, rdivide)\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrdivide\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtrue_divide\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrue_divide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "b / np.reshape(priors, (vocab_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10209.0"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "access_bigram_matrix('the', 'food', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<33726x33726 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 675716 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def most_probable_next_word(bigram, word, max_words=10):\n",
    "    next_words = []\n",
    "    for key in bigram.keys():\n",
    "        if word == key[0]:\n",
    "            next_words.append((bigram[key], key[1]))\n",
    "    next_words.sort(reverse=True)\n",
    "    return list(map(lambda x: x[1], next_words[0:max_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['but', 'and', 'the', 'i', 'as', 'food', 'thing', 'it', 'for', 'too']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_probable_next_word(bigram, 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "I = np.array([0,0,1,3,1,0,0])\n",
    "J = np.array([0,2,1,3,1,0,0])\n",
    "V = np.array([1,1,1,1,1,1,1])\n",
    "B = sparse.coo_matrix((V,(I,J)),shape=(4,4)).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.        ,         nan,  0.33333333,         nan],\n",
       "        [        nan,  1.        ,         nan,         nan],\n",
       "        [        nan,         nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan,  1.        ]])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B / sparse.csr_matrix([[3, 3, 3, 3], [2, 2, 2, 2], [1, 1, 1, 1], [1, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "B.data = B.data / np.array([[3], [2], [1], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "B = B.multiply(sparse.csr_matrix([[1/3], [1/2], [1], [1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ,  0.33333333,  0.        ],\n",
       "       [ 0.        ,  1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
